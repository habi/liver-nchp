{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the 'data' of the fishes\n",
    "Wrestle with the data, check parameters and generate some helping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask temporarry files go to /media/habi/Fast_SSD/tmp\n"
     ]
    }
   ],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporarry files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/habi/miniconda3/lib/python3.9/site-packages/distributed/node.py:181: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 46433 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can seee what DASK is doing at \"http://localhost:46433/status\"\n"
     ]
    }
   ],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from /home/habi/research-storage-iee\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "overthere = True # Load the data directly from the iee-research_storage drive\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'EAWAG')\n",
    "if overthere:       \n",
    "    if 'Linux' in platform.system():\n",
    "        Root = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    else:\n",
    "        Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize=None    \n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectionsize(logfile):\n",
    "    \"\"\"How big did we set the camera?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number Of Rows' in line:\n",
    "                y = int(line.split('=')[1])\n",
    "            if 'Number Of Columns' in line:\n",
    "                x = int(line.split('=')[1])                \n",
    "    return(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(logfile):\n",
    "    \"\"\"Get the filter we used whole scanning from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Filter=' in line:\n",
    "                whichfilter = line.split('=')[1].strip()\n",
    "    return(whichfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposuretime(logfile):\n",
    "    \"\"\"Get the exposure time size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Exposure' in line:\n",
    "                exposuretime = int(line.split('=')[1])\n",
    "    return(exposuretime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    grayvalue = None\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotationstep(logfile):\n",
    "    \"\"\"Get the rotation step from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Rotation Step' in line:\n",
    "                rotstep = float(line.split('=')[1])\n",
    "    return(rotstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frameaveraging(logfile):\n",
    "    \"\"\"Get the frame averaging from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Averaging' in line:\n",
    "                avg = line.split('=')[1]\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine(logfile):\n",
    "    \"\"\"Get the machine we used to scan\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scanner' in line:\n",
    "                machine = line.split('=')[1].strip()\n",
    "    return(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1].strip()\n",
    "    return(pandas.to_timedelta(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    stacks = 1\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the fish?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring , format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "                (date)\n",
    "    return(date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                               recursive=True),\n",
    "                                     key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "- 105005_104015/proj/105005_104015~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015/proj/105005_104015~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015/proj/105005_104015~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015/proj/105005_104015~03.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015/proj/105005_104015~04.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015/proj/105005_104015.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645/proj/104671_156645~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645/proj/104671_156645~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645/proj/104671_156645~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645/proj/104671_156645.log is missing matching reconstructions\n",
      "[]\n",
      "- 103761/proj_oj/103761.log is missing matching reconstructions\n"
     ]
    }
   ],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/habi/research-storage-iee/103908/jaw/rec...</td>\n",
       "      <td>/home/habi/research-storage-iee/103908/jaw/rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/W/rec_al...</td>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/W/rec_al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/W/rec_no...</td>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/W/rec_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/P/rec_al...</td>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/P/rec_al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/P/rec_no...</td>\n",
       "      <td>/home/habi/research-storage-iee/Teeth/P/rec_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>/home/habi/research-storage-iee/103723/head/re...</td>\n",
       "      <td>/home/habi/research-storage-iee/103723/head/rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>/home/habi/research-storage-iee/103723/rec_oj/...</td>\n",
       "      <td>/home/habi/research-storage-iee/103723/rec_oj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>/home/habi/research-storage-iee/103767/head/re...</td>\n",
       "      <td>/home/habi/research-storage-iee/103767/head/rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>/home/habi/research-storage-iee/103767/rec_oj/...</td>\n",
       "      <td>/home/habi/research-storage-iee/103767/rec_oj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>/home/habi/research-storage-iee/103767/rec_oj_...</td>\n",
       "      <td>/home/habi/research-storage-iee/103767/rec_oj_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               LogFile  \\\n",
       "0    /home/habi/research-storage-iee/103908/jaw/rec...   \n",
       "1    /home/habi/research-storage-iee/Teeth/W/rec_al...   \n",
       "2    /home/habi/research-storage-iee/Teeth/W/rec_no...   \n",
       "3    /home/habi/research-storage-iee/Teeth/P/rec_al...   \n",
       "4    /home/habi/research-storage-iee/Teeth/P/rec_no...   \n",
       "..                                                 ...   \n",
       "288  /home/habi/research-storage-iee/103723/head/re...   \n",
       "289  /home/habi/research-storage-iee/103723/rec_oj/...   \n",
       "290  /home/habi/research-storage-iee/103767/head/re...   \n",
       "291  /home/habi/research-storage-iee/103767/rec_oj/...   \n",
       "292  /home/habi/research-storage-iee/103767/rec_oj_...   \n",
       "\n",
       "                                                Folder  \n",
       "0       /home/habi/research-storage-iee/103908/jaw/rec  \n",
       "1    /home/habi/research-storage-iee/Teeth/W/rec_al...  \n",
       "2    /home/habi/research-storage-iee/Teeth/W/rec_no...  \n",
       "3    /home/habi/research-storage-iee/Teeth/P/rec_al...  \n",
       "4    /home/habi/research-storage-iee/Teeth/P/rec_no...  \n",
       "..                                                 ...  \n",
       "288    /home/habi/research-storage-iee/103723/head/rec  \n",
       "289      /home/habi/research-storage-iee/103723/rec_oj  \n",
       "290    /home/habi/research-storage-iee/103767/head/rec  \n",
       "291      /home/habi/research-storage-iee/103767/rec_oj  \n",
       "292    /home/habi/research-storage-iee/103767/rec_oj_2  \n",
       "\n",
       "[293 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (263294990.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_738907/263294990.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    asdfasdf==\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "asdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/237: Scan 103908\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "2/237: Scan Teeth\\W_rec_al0.25 does not contain any reconstructions and will be removed in the next step\n",
      "3/237: Scan Teeth\\W_rec_nofilter does not contain any reconstructions and will be removed in the next step\n",
      "4/237: Scan Teeth\\P_rec_al0.25 does not contain any reconstructions and will be removed in the next step\n",
      "5/237: Scan Teeth\\P_rec_nofilter does not contain any reconstructions and will be removed in the next step\n",
      "6/237: Scan 104016\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "7/237: Scan 103375\\rec_stuck does not contain any reconstructions and will be removed in the next step\n",
      "8/237: Scan 103375\\rec does not contain any reconstructions and will be removed in the next step\n",
      "9/237: Scan NY75\\rec does not contain any reconstructions and will be removed in the next step\n",
      "10/237: Scan 161543\\head_30um_rec does not contain any reconstructions and will be removed in the next step\n",
      "11/237: Scan 161543\\rec does not contain any reconstructions and will be removed in the next step\n",
      "12/237: Scan 10628\\head_13um_rec does not contain any reconstructions and will be removed in the next step\n",
      "13/237: Scan 14298\\rec does not contain any reconstructions and will be removed in the next step\n",
      "14/237: Scan 14269\\rec does not contain any reconstructions and will be removed in the next step\n",
      "15/237: Scan 103754\\rec does not contain any reconstructions and will be removed in the next step\n",
      "16/237: Scan 21322\\jaw_rec_mouth_5um does not contain any reconstructions and will be removed in the next step\n",
      "17/237: Scan 131282\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "18/237: Scan 131282\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "19/237: Scan 10628\\head_18um_rec does not contain any reconstructions and will be removed in the next step\n",
      "20/237: Scan 11045\\rec does not contain any reconstructions and will be removed in the next step\n",
      "21/237: Scan 13115\\rec_13um does not contain any reconstructions and will be removed in the next step\n",
      "22/237: Scan 103571\\rec does not contain any reconstructions and will be removed in the next step\n",
      "23/237: Scan 14128\\rec does not contain any reconstructions and will be removed in the next step\n",
      "24/237: Scan 105105\\rec does not contain any reconstructions and will be removed in the next step\n",
      "25/237: Scan 103767\\rec does not contain any reconstructions and will be removed in the next step\n",
      "26/237: Scan 103908\\stack_rec does not contain any reconstructions and will be removed in the next step\n",
      "27/237: Scan 106985\\rec does not contain any reconstructions and will be removed in the next step\n",
      "28/237: Scan TJ3\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "29/237: Scan KAT13\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "30/237: Scan KAT13\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "31/237: Scan 104016\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "32/237: Scan KC31\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "33/237: Scan 103778\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "34/237: Scan 103635\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "35/237: Scan 103734\\rec does not contain any reconstructions and will be removed in the next step\n",
      "36/237: Scan 103761\\rec does not contain any reconstructions and will be removed in the next step\n",
      "37/237: Scan 10715\\rec does not contain any reconstructions and will be removed in the next step\n",
      "38/237: Scan 11447\\rec does not contain any reconstructions and will be removed in the next step\n",
      "39/237: Scan 105005_104015\\104015_rec does not contain any reconstructions and will be removed in the next step\n",
      "40/237: Scan 105005_104015\\105005_rec does not contain any reconstructions and will be removed in the next step\n",
      "41/237: Scan 103635\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "42/237: Scan 103778\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "43/237: Scan 104021\\rec does not contain any reconstructions and will be removed in the next step\n",
      "44/237: Scan 104621\\rec does not contain any reconstructions and will be removed in the next step\n",
      "45/237: Scan 11116\\rec does not contain any reconstructions and will be removed in the next step\n",
      "46/237: Scan 11946\\rec does not contain any reconstructions and will be removed in the next step\n",
      "47/237: Scan 11639\\rec does not contain any reconstructions and will be removed in the next step\n",
      "48/237: Scan 13393\\rec does not contain any reconstructions and will be removed in the next step\n",
      "49/237: Scan BH58\\rec does not contain any reconstructions and will be removed in the next step\n",
      "50/237: Scan 11500\\rec does not contain any reconstructions and will be removed in the next step\n",
      "51/237: Scan 10448\\rec does not contain any reconstructions and will be removed in the next step\n",
      "52/237: Scan 11729\\rec does not contain any reconstructions and will be removed in the next step\n",
      "53/237: Scan 11322\\rec does not contain any reconstructions and will be removed in the next step\n",
      "54/237: Scan 11053\\rec does not contain any reconstructions and will be removed in the next step\n",
      "55/237: Scan 109209\\rec does not contain any reconstructions and will be removed in the next step\n",
      "56/237: Scan 109220\\rec does not contain any reconstructions and will be removed in the next step\n",
      "57/237: Scan 161476\\rec does not contain any reconstructions and will be removed in the next step\n",
      "58/237: Scan 104042\\rec does not contain any reconstructions and will be removed in the next step\n",
      "59/237: Scan 103658\\rec does not contain any reconstructions and will be removed in the next step\n",
      "60/237: Scan 10628\\full_188um_rec does not contain any reconstructions and will be removed in the next step\n",
      "61/237: Scan 10791\\rec does not contain any reconstructions and will be removed in the next step\n",
      "62/237: Scan 11313\\rec does not contain any reconstructions and will be removed in the next step\n",
      "63/237: Scan 106816\\rec does not contain any reconstructions and will be removed in the next step\n",
      "64/237: Scan 11601\\rec does not contain any reconstructions and will be removed in the next step\n",
      "65/237: Scan 104671_156645\\104671_rec does not contain any reconstructions and will be removed in the next step\n",
      "66/237: Scan 10618\\head_head_rec does not contain any reconstructions and will be removed in the next step\n",
      "67/237: Scan 10618\\rec does not contain any reconstructions and will be removed in the next step\n",
      "68/237: Scan 104671_156645\\156645_rec does not contain any reconstructions and will be removed in the next step\n",
      "69/237: Scan 109320\\rec does not contain any reconstructions and will be removed in the next step\n",
      "70/237: Scan IG161\\rec does not contain any reconstructions and will be removed in the next step\n",
      "71/237: Scan 10605\\rec does not contain any reconstructions and will be removed in the next step\n",
      "72/237: Scan 10576\\rec does not contain any reconstructions and will be removed in the next step\n",
      "73/237: Scan IG104\\rec does not contain any reconstructions and will be removed in the next step\n",
      "74/237: Scan JU22\\rec does not contain any reconstructions and will be removed in the next step\n",
      "75/237: Scan 104061\\rec does not contain any reconstructions and will be removed in the next step\n",
      "76/237: Scan TNB1\\rec does not contain any reconstructions and will be removed in the next step\n",
      "77/237: Scan 11344\\rec does not contain any reconstructions and will be removed in the next step\n",
      "78/237: Scan 10794\\rec does not contain any reconstructions and will be removed in the next step\n",
      "79/237: Scan 11965\\rec does not contain any reconstructions and will be removed in the next step\n",
      "80/237: Scan 14125\\rec does not contain any reconstructions and will be removed in the next step\n",
      "81/237: Scan BI10\\rec does not contain any reconstructions and will be removed in the next step\n",
      "82/237: Scan 12319\\rec does not contain any reconstructions and will be removed in the next step\n",
      "83/237: Scan 13492\\rec does not contain any reconstructions and will be removed in the next step\n",
      "84/237: Scan BI10\\rec does not contain any reconstructions and will be removed in the next step\n",
      "85/237: Scan TMG15\\rec does not contain any reconstructions and will be removed in the next step\n",
      "86/237: Scan KI30\\rec does not contain any reconstructions and will be removed in the next step\n",
      "87/237: Scan MA31\\rec does not contain any reconstructions and will be removed in the next step\n",
      "88/237: Scan 11344\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "89/237: Scan 10794\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "90/237: Scan 11965\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "91/237: Scan MA31\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "92/237: Scan 12849\\rec does not contain any reconstructions and will be removed in the next step\n",
      "93/237: Scan 104061\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "94/237: Scan IG156\\rec does not contain any reconstructions and will be removed in the next step\n",
      "95/237: Scan 103761\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "96/237: Scan 103767\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "97/237: Scan 11447\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "98/237: Scan 10448\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "99/237: Scan 10151\\rec does not contain any reconstructions and will be removed in the next step\n",
      "100/237: Scan IG291\\rec does not contain any reconstructions and will be removed in the next step\n",
      "101/237: Scan 13069\\rec does not contain any reconstructions and will be removed in the next step\n",
      "102/237: Scan 13281\\rec does not contain any reconstructions and will be removed in the next step\n",
      "103/237: Scan 14233\\rec does not contain any reconstructions and will be removed in the next step\n",
      "104/237: Scan 13269\\rec does not contain any reconstructions and will be removed in the next step\n",
      "105/237: Scan MB51\\rec does not contain any reconstructions and will be removed in the next step\n",
      "106/237: Scan TS03\\rec does not contain any reconstructions and will be removed in the next step\n",
      "107/237: Scan IG142\\rec does not contain any reconstructions and will be removed in the next step\n",
      "108/237: Scan 103723\\rec does not contain any reconstructions and will be removed in the next step\n",
      "109/237: Scan MA38\\rec does not contain any reconstructions and will be removed in the next step\n",
      "110/237: Scan 106641\\rec does not contain any reconstructions and will be removed in the next step\n",
      "111/237: Scan 11992\\rec does not contain any reconstructions and will be removed in the next step\n",
      "112/237: Scan 11729\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "113/237: Scan 12807\\rec does not contain any reconstructions and will be removed in the next step\n",
      "114/237: Scan 13420\\rec does not contain any reconstructions and will be removed in the next step\n",
      "115/237: Scan 11601\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "116/237: Scan IG41\\rec does not contain any reconstructions and will be removed in the next step\n",
      "117/237: Scan ZuOS148\\rec does not contain any reconstructions and will be removed in the next step\n",
      "118/237: Scan 161476\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "119/237: Scan ZU12\\rec does not contain any reconstructions and will be removed in the next step\n",
      "120/237: Scan 11807\\rec does not contain any reconstructions and will be removed in the next step\n",
      "121/237: Scan AN33\\rec does not contain any reconstructions and will be removed in the next step\n",
      "122/237: Scan 11557\\rec does not contain any reconstructions and will be removed in the next step\n",
      "123/237: Scan ZU05\\rec does not contain any reconstructions and will be removed in the next step\n",
      "124/237: Scan 12319\\head_50um_rec does not contain any reconstructions and will be removed in the next step\n",
      "125/237: Scan 12319\\head_30um_rec does not contain any reconstructions and will be removed in the next step\n",
      "126/237: Scan 109188\\head_rec does not contain any reconstructions and will be removed in the next step\n",
      "127/237: Scan IG80\\rec does not contain any reconstructions and will be removed in the next step\n",
      "128/237: Scan IG80\\head_rec does not contain any reconstructions and will be removed in the next step\n",
      "129/237: Scan 10448\\head_rec_head does not contain any reconstructions and will be removed in the next step\n",
      "130/237: Scan 10448\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "131/237: Scan 11601\\head_rec_head does not contain any reconstructions and will be removed in the next step\n",
      "132/237: Scan ZU12\\rec_reconstruct does not contain any reconstructions and will be removed in the next step\n",
      "133/237: Scan TS03\\rec_rereconstruct does not contain any reconstructions and will be removed in the next step\n",
      "134/237: Scan IG104\\head_rec_head does not contain any reconstructions and will be removed in the next step\n",
      "135/237: Scan AN33\\head_rec_head does not contain any reconstructions and will be removed in the next step\n",
      "136/237: Scan 109320\\head_rec does not contain any reconstructions and will be removed in the next step\n",
      "137/237: Scan 10605\\head_rec does not contain any reconstructions and will be removed in the next step\n",
      "138/237: Scan 10794\\rec_rescan_rereconstruct_OJ does not contain any reconstructions and will be removed in the next step\n",
      "139/237: Scan IG80\\rec_rereconstruct_OJ does not contain any reconstructions and will be removed in the next step\n",
      "140/237: Scan 11557\\rec_rereconstruct_OJ_A does not contain any reconstructions and will be removed in the next step\n",
      "141/237: Scan 11557\\rec_rereconstruct_OJ_B does not contain any reconstructions and will be removed in the next step\n",
      "142/237: Scan MA31\\rec_rescan_rereconstruct_OJ does not contain any reconstructions and will be removed in the next step\n",
      "143/237: Scan 10619\\rec_rescan_OJ does not contain any reconstructions and will be removed in the next step\n",
      "144/237: Scan 10619\\rec does not contain any reconstructions and will be removed in the next step\n",
      "145/237: Scan 10619\\head_rec_unbinned_17.5um does not contain any reconstructions and will be removed in the next step\n",
      "146/237: Scan 10619\\head_rec_2xbin_23um does not contain any reconstructions and will be removed in the next step\n",
      "147/237: Scan 10619\\head_rec_4xbin_40um does not contain any reconstructions and will be removed in the next step\n"
     ]
    }
   ],
   "source": [
    "# Get the file names of the reconstructions\n",
    "for c, row in Data[Data['Number of reconstructions'] == 0].iterrows():\n",
    "    print('%s/%s: Scan %s does not contain any reconstructions and '\n",
    "          'will be removed in the next step' % (c+1, len(Data), os.path.join(row.Fish, row.Scan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 237 folders in total\n",
      "We have 90 folders *with* reconstructions in them\n"
     ]
    }
   ],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "print('We have %s folders in total' % (len(Data)))\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders *with* reconstructions in them' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [get_filter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [get_exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [get_machine(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [get_frameaveraging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [get_projectionsize(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [get_rotationstep(log) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [round((ps ** 0.5)/100)*100  for ps in Data['ProjectionSize']]\n",
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [ st * stk  for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file for each rec-folder, in which we can note what's going on with the fish\n",
    "# Generate filename\n",
    "for c,row in Data.iterrows():\n",
    "    Data.at[c, 'CommentFile'] = os.path.join(os.path.dirname(row.Folder),\n",
    "                                              row.Fish + '.' + row.Scan + '.md')\n",
    "# Create actual file on disk\n",
    "for c,row in Data.iterrows():\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(row.CommentFile):\n",
    "        with open(row.CommentFile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder') \n",
    "# columns.remove('Fish')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80*'-')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check voxel sizes (*rounded* to two after-comma values)\n",
    "# # If different, spit out which values\n",
    "# roundto = 2\n",
    "# if len(Data['Voxelsize'].round(roundto).unique()) > 1:\n",
    "#     print('We scanned all datasets with %s different voxel sizes' % len(Data['Voxelsize'].round(roundto).unique()))\n",
    "#     for vs in sorted(Data['Voxelsize'].round(roundto).unique()):\n",
    "#         print('-', vs, 'um for ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(vs) == round(row['Voxelsize'], roundto):\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We scanned all datasets with equal voxel size, namely %s um.' % float(Data['Voxelsize'].round(roundto).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(Data['Grayvalue'].unique()) > 1:\n",
    "#     print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "#     for gv in Data['Grayvalue'].unique():\n",
    "#         print(gv, 'for Samples ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(gv) == row['Grayvalue']:\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Fish', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we scanned for 345 hours and 36 minutes)\n",
      "\t - Of these, we scanned 157 hours and 38 minutes on the SkyScan2214,for 71 scans\n",
      "\t - Of these, we scanned 187 hours and 57 minutes on the SkyScan1272,for 19 scans\n"
     ]
    }
   ],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds,60*60)\n",
    "minutes, seconds = divmod(remainder,60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds,60*60)\n",
    "    minutes, seconds = divmod(remainder,60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          'for %s scans' % (hours,\n",
    "                            minutes,\n",
    "                            machine,\n",
    "                            len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow', 'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel('Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel(os.path.join(Root,'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in D:\\Results\\EAWAG\\X_ArchiveFiles\\02.07.2021_CTscanFishList.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read Mikkis datafile\n",
    "MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "# Read excel file and use the first column as index\n",
    "print('Reading in %s' % MikkisFile)\n",
    "DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fishec</th>\n",
       "      <th>FieldID</th>\n",
       "      <th>OtherID</th>\n",
       "      <th>ReplacementID</th>\n",
       "      <th>Length(cm)</th>\n",
       "      <th>TemporaryJar</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>Ecology</th>\n",
       "      <th>Scan date</th>\n",
       "      <th>HeadScan</th>\n",
       "      <th>OralJawScan</th>\n",
       "      <th>PharyngealJawScan</th>\n",
       "      <th>OperculumVisible</th>\n",
       "      <th>DataUploaded</th>\n",
       "      <th>QualityChecked</th>\n",
       "      <th>ScanComments</th>\n",
       "      <th>SpecimenReturned</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp blue</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-02-08T12:25:19</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-3 inner row of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp blue</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-02-08T14:24:12</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-3 inner row of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Enterochromis I</td>\n",
       "      <td>cinctus (St. E)</td>\n",
       "      <td>detritivore</td>\n",
       "      <td>2021-02-04T11:21:23</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>not complete</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pharyngeal jaw not complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Enterochromis I</td>\n",
       "      <td>cinctus (St. E)</td>\n",
       "      <td>detritivore</td>\n",
       "      <td>2021-02-04T13:30:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Incertae sedis</td>\n",
       "      <td>thick skin</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bad segmentation quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fishec FieldID OtherID ReplacementID Length(cm) TemporaryJar  \\\n",
       "0  103635     NaN     NaN           NaN        < 7        < 7cm   \n",
       "1  103635     NaN     NaN           NaN        < 7        < 7cm   \n",
       "2  104016     NaN     NaN           NaN        < 7        < 7cm   \n",
       "3  104016     NaN     NaN           NaN        < 7        < 7cm   \n",
       "4   14298     NaN     NaN           NaN        < 7        < 7cm   \n",
       "\n",
       "             Genus            Species      Ecology            Scan date  \\\n",
       "0  \"Astatotilapia\"  nubila swamp blue  insectivore  2021-02-08T12:25:19   \n",
       "1  \"Astatotilapia\"  nubila swamp blue  insectivore  2021-02-08T14:24:12   \n",
       "2  Enterochromis I    cinctus (St. E)  detritivore  2021-02-04T11:21:23   \n",
       "3  Enterochromis I    cinctus (St. E)  detritivore  2021-02-04T13:30:11   \n",
       "4   Incertae sedis         thick skin  insectivore                  NaN   \n",
       "\n",
       "           HeadScan OralJawScan PharyngealJawScan  OperculumVisible  \\\n",
       "0  no 20um headscan         yes               yes  no 20um headscan   \n",
       "1  no 20um headscan         yes               yes  no 20um headscan   \n",
       "2                no         yes      not complete                no   \n",
       "3               NaN         NaN               NaN               NaN   \n",
       "4                no         yes               NaN                no   \n",
       "\n",
       "  DataUploaded QualityChecked                      ScanComments  \\\n",
       "0          NaN            NaN  2-3 inner row of tricuspid teeth   \n",
       "1          NaN            NaN  2-3 inner row of tricuspid teeth   \n",
       "2          NaN            NaN       pharyngeal jaw not complete   \n",
       "3          NaN            NaN                               NaN   \n",
       "4          NaN            NaN          bad segmentation quality   \n",
       "\n",
       "   SpecimenReturned Comments  \n",
       "0               NaN      NaN  \n",
       "1               NaN      NaN  \n",
       "2               NaN      NaN  \n",
       "3               NaN      NaN  \n",
       "4               NaN      NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we look at and display all the info we know about it\n",
    "# Set a substring you're looking for to the variable below\n",
    "# In which jar can we find it?\n",
    "fish = '104061'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: The fish 13405/104061/ should now go in jar \"length=14.5 cm\" (Mark5))\n"
     ]
    }
   ],
   "source": [
    "# In which jar should it be/go?\n",
    "foundfishes = 0\n",
    "for d, row in DataMikki.iterrows():\n",
    "    if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "    (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "        foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "        # remove nan from the list of hits\n",
    "        foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "        print('*%s*: The fish ' % fish, end='')        \n",
    "        if len(foundfishes) > 1:\n",
    "            for found in foundfishes:\n",
    "                print(found.upper(), end='/')\n",
    "        else:\n",
    "            print(foundfishes[0].upper(), end='')\n",
    "        print(' should now go in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                              row['TemporaryJar']))\n",
    "if not foundfishes:\n",
    "    print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: Found on disk in D:\\Results\\EAWAG\\104061\n"
     ]
    }
   ],
   "source": [
    "# Do we have something from this fish on disk?\n",
    "ondisk = glob.glob(os.path.join(Root, '*%s*' % fish))\n",
    "if len(ondisk):\n",
    "    for found in ondisk:\n",
    "        print('*%s*: Found on disk in %s' % (fish, found))\n",
    "        foundondisk = 1\n",
    "else:\n",
    "    print('*%s*: Nothing found in %s' % (fish, Root))\n",
    "    foundondisk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: We have a folder (D:\\Results\\EAWAG\\104061) for this sample, but nothing in the dataframe, so it probably is all good\n",
      "Check the folder to be shure\n"
     ]
    }
   ],
   "source": [
    "# Did we scan it already?\n",
    "found = 0\n",
    "for c, row in Data.iterrows():\n",
    "    if fish in row.Fish:\n",
    "        print('*%s*: Sample %s/%s was scanned on %s' % (fish, row['Fish'], row['Scan'], row['Scan date']))\n",
    "        found = 1\n",
    "if not found:\n",
    "    if foundondisk:\n",
    "        print('*%s*: We have a folder (%s) for this sample, but nothing in the dataframe, so it probably is all good' % (fish, ondisk[0]))\n",
    "        print('Check the folder to be shure')\n",
    "    else:\n",
    "        print('*%s*: Nothing about this sample is found in our dataframe' % fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104061, Labrochromis sp. \"stone\" (pharyngeal mollusc crusher), head cropped\n"
     ]
    }
   ],
   "source": [
    "# Can we find it in FullHeadList.txt?\n",
    "def findinFullHeadList(sample):\n",
    "    ''' Look for the sample in the FullHeadList.txt file'''\n",
    "    fullheadlist = glob.glob(os.path.join(Root, 'FullHeadList.*'))[0]    \n",
    "    found = 0\n",
    "    with open(fullheadlist, 'r') as f:\n",
    "        for line in f:\n",
    "            if str(sample) in line:\n",
    "                print(line.strip())\n",
    "                found = 1\n",
    "    if not found:\n",
    "        return('*%s*: Nothing found in %s' % (sample, fullheadlist))\n",
    "    else:\n",
    "        return(None)\n",
    "findinFullHeadList(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found these comment files in our dataframe\n",
      "--------------------------------------------------------------------------------\n",
      "- D:\\Results\\EAWAG\\104061\\104061.rec.md\n",
      "----------\n",
      "# Fish 104061, Scan rec\n",
      "\n",
      "This fish was scanned on 2021-07-15T15:23:55 on the SkyScan2214, with a voxel size of 8.95 μm.\n",
      "\n",
      "## Comments\n",
      "--------------------------------------------------------------------------------\n",
      "- D:\\Results\\EAWAG\\104061\\104061.rec_rescan.md\n",
      "----------\n",
      "# Fish 104061, Scan rec_rescan\n",
      "BEEEEP!\n",
      "\n",
      "This fish was scanned on 2021-08-20T11:06:31 on the SkyScan2214, with a voxel size of 10.0 μm.\n",
      "\n",
      "## Comments\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Do we need to rescan this fish\n",
    "# Find all relevant comment files\n",
    "commentfiles = glob.glob(os.path.join(Root, '*%s*' % fish, '**', '*.md'), recursive=True)\n",
    "print('We found these comment files in our dataframe')\n",
    "for c, row in Data.iterrows():\n",
    "    if fish in row.Fish:\n",
    "        print('\\t-', row.CommentFile)\n",
    "        found = 1\n",
    "print(80*'-')\n",
    "if len(commentfiles):\n",
    "    for commentfile in commentfiles:\n",
    "        print('-', commentfile)\n",
    "        print(10*'-')\n",
    "        with open(commentfile, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                print(line.strip())\n",
    "                if 'rescan' in line:\n",
    "                    print('BEEEEP!')\n",
    "        print(80*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 of the fishes need complete head scans.\n",
    "Let's try to go through Mikkis/Kassandras list and see how far we progressed through that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHERE is missing a head-scan on disk, but is found on line 12 of the full head list\n",
      "WHERE is missing a head-scan on disk, but is found on line 13 of the full head list\n",
      "105105 is missing a head-scan on disk, but is found on line 94 of the full head list\n"
     ]
    }
   ],
   "source": [
    "# Read in full head list, go through all the scans we alredy did and see what needs to be done\n",
    "fullheadlist = glob.glob(os.path.join(Root, '*Head*.txt'))[0]\n",
    "HeadsToBeScanned = []\n",
    "with open(fullheadlist, 'r', encoding='utf-8') as file:\n",
    "    headdone = False\n",
    "    for ln, line in enumerate(file):\n",
    "        if line.strip():  #skip empty lines\n",
    "            # The first 'item' on the line should be the fish ID\n",
    "            fish = line.strip().split()[0].replace(',','').upper()\n",
    "            # Let's ignore some lines which don't start with a fish ID\n",
    "            # The set-join here removes duplicate characters from the string (e.g. =====, !! and ::)\n",
    "            if len(''.join(set(fish))) > 2:\n",
    "                for c, row in Data[Data.Fish == fish].iterrows():\n",
    "                    if 'head' in row.Scan:\n",
    "                        # print('\\t%s has a head-scan' % row.Fish)\n",
    "                        # print('%s has a head-scan on disk, and is found on line %s of the full head list' % (fish, ln + 1 ))\n",
    "                        headdone = True\n",
    "                    else:\n",
    "                        headdone = False\n",
    "                # At this point we have either found the fish in the list or 'headdone' is false\n",
    "                if not headdone:\n",
    "                    print('%s is missing a head-scan on disk, but is found on line %s of the full head list' % (fish, ln + 1 ))\n",
    "                    HeadsToBeScanned.append(fish)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fish 10448 can be ignored because we did another scan after the head-scan, so we reset \"headdone\" in the loop above\n",
    "# We could probably do it in a more clever way, but already spent too much time on this part :)\n",
    "try:\n",
    "    HeadsToBeScanned.remove('10448')\n",
    "    # HeadsToBeScanned.remove('105515')\n",
    "except ValueError:\n",
    "    # Nothing to see here, pass along\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*WHERE*: Nothing found in D:\\Results\\EAWAG\\X_ArchiveFiles\\02.07.2021_CTscanFishList.xlsx\n",
      "*WHERE*: Nothing found in D:\\Results\\EAWAG\\X_ArchiveFiles\\02.07.2021_CTscanFishList.xlsx\n",
      "*105105*: A fish called 105105 should be found in jar \"length=15 cm\" (Mark1))\n"
     ]
    }
   ],
   "source": [
    "for fish in HeadsToBeScanned:\n",
    "    # In which jar should we look for the fishes we still need to scan the head of?\n",
    "    foundfishes = 0\n",
    "    for d, row in DataMikki.iterrows():\n",
    "        if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "        (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "            foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "            # remove nan from the list of hits\n",
    "            foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "            print('*%s*: A fish called ' % fish, end='')        \n",
    "            if len(foundfishes) > 1:\n",
    "                for found in foundfishes:\n",
    "                    print(found.upper(), end='/')\n",
    "            else:\n",
    "                print(foundfishes[0].upper(), end='')\n",
    "            print(' should be found in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                    row['TemporaryJar']))\n",
    "    if not foundfishes:\n",
    "        print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 10448\\10448.rec.md - According to Mikki \"try re reconstructing to better later segmentation, OJ artifacts, PJ is good\", so we 'only' optimize on oral jaw.\n",
      "- 10628\\head_13um\\10628.rec.md Needs to be rescanned, according to XLS sheet from Mikki\n",
      "- 109188\\109188.rec.md 2.11.21, DH: The fish was not aligned nicely perpendicular in the sample holder. I re-reconstructed the data *without* a ROI. Parts of the OJ might still be outside of the visible region. @Mikki, can you double-check?\n",
      "- 11807\\11807.rec.md - Mikki and David need to discuss this in detail.\n",
      "- IG92\\IG92.rec.md Needs to be rescanned, according to Mikki not better through re-reconstruction.\n",
      "- IG96\\IG96.rec.md According to Mikki not good, even if re-reconstructed.\n",
      "- ZuOS148\\ZuOS148.rec.md which Mikki will update in the encompassing Excel sheet as ZuOS148. 21.10.2021 DH\n"
     ]
    }
   ],
   "source": [
    "# Some of the reconstructions need to be looked at?\n",
    "# Mikki wrote something about this into the files.\n",
    "# Get a list of *all* comment files\n",
    "CommentFiles = glob.glob(os.path.join(Root, '**', '*.md'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through all the 239 comments files we find\n",
      "  0/239: 10151\\10151.rec.md: 08.02.2022 - ML - Quality check PNG uploaded to the folder. The artifacts are pretty bad, not sure if re alignment will fix it entirely\n",
      " 20/239: 103761\\103761.rec.md: ML 16.11.2021:  OJ and PJ need rescan at lower voxel where possible.\n",
      " 23/239: 103767\\103767.rec.md: ML 16.11.2021 - Tried re reconstructing, OJ has artifacts connected the jaws. Please rescan just the OJ at lower voxel size if possible.\n",
      " 38/239: 10448\\10448.rec.md: - According to Mikki \"try re reconstructing to better later segmentation, OJ artifacts, PJ is good\", so we 'only' optimize on oral jaw.\n",
      " 64/239: 10618\\10618.head_rec.md: ML 15.11.2021: Edge of operculum missing in the head scan\n",
      " 67/239: 10619\\10619.rec.md: ML 15.11.2021: Needs a full head scan\n",
      " 75/239: 10628\\head_13um\\10628.rec.md: Needs to be rescanned, according to XLS sheet from Mikki\n",
      " 75/239: 10628\\head_13um\\10628.rec.md: ML 15.11.2021:  Reconfirming, needs rescanning - for full head, OJ and PJ.\n",
      " 76/239: 10628\\head_18um\\10628.head_18um_rec.md: ML 15.11.2021:  Head not complete - operculum is cut off\n",
      " 92/239: 10794\\10794.rec_rescan.md: OJ still has artifacts - 21.09.2021, ML\n",
      " 95/239: 109188\\109188.rec.md: 02.11.21, DH: The fish was not aligned nicely perpendicular in the sample holder. I re-reconstructed the data *without* a ROI. Parts of the OJ might still be outside of the visible region. @Mikki, can you double-check\n",
      " 95/239: 109188\\109188.rec.md: 01.02.2022, ML: Sorry I didn't see your text. The scan is still cropped on the specimen's OJ, left side. Will need rescanning of the OJ.\n",
      "109/239: 11116\\11116.rec.md: ML 15.11.2021: Head scan - top of the skull the 'mohawk bone' (supraoccipital bone) is cropped. So close! but not complete\n",
      "132/239: 11729\\11729.rec.md: ML 15.11.2021 - confirmation segementation is much better.\n",
      "135/239: 11807\\11807.rec.md: - Mikki and David need to discuss this in detail\n",
      "136/239: 11807\\head\\11807.head_rec.md: ML 15.11.2021: Head scan has cropped operculum\n",
      "141/239: 11965\\11965.rec_rescan.md: Oral jaw has artifacts in segmentation, can we try re reconstruction on the rescan - 21.09.2021, ML\n",
      "143/239: 11992\\11992.rec.md: 08.02.2022 - ML - can realignment be done with focus on the PJ?\n",
      "143/239: 11992\\11992.rec.md: 15.02.2022 - DH - Tried realignment, but it looks like there is some movemen artefacts in the PJ. Needs rescanning.\n",
      "162/239: 13420\\13420.rec.md: ML 02.11.2021: Files within the rec folder are named 12807\n",
      "162/239: 13420\\13420.rec.md: ML: Thanks :)\n",
      "201/239: IG92\\IG92.rec.md: Needs to be rescanned, according to Mikki not better through re-reconstruction.\n",
      "203/239: IG96\\IG96.rec.md: According to Mikki not good, even if re-reconstructed.\n",
      "234/239: ZU05\\ZU05.rec.md: ML 15.11.2021: \"312\" specimen is actually ZuOS148 - Therefore this \"ZU05\" is likely to be ZuOS115 - need to confirm\n",
      "238/239: ZuOS148\\ZuOS148.rec.md: which Mikki will update in the encompassing Excel sheet as ZuOS148. 21.10.2021 DH\n"
     ]
    }
   ],
   "source": [
    "# Read what we want\n",
    "print('Going through all the %s comments files we find' % len(CommentFiles))\n",
    "for c, cf in enumerate(CommentFiles):\n",
    "    with open(cf, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if 'Mikki' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root)+1:], line.strip()))\n",
    "            elif 'ML' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root)+1:], line.strip()))\n",
    "            elif 'realign' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root)+1:], line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sort_values(['Scan date'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
